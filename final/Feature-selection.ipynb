{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d99098fcbfa6a9ec3ca72e4f681233c323c255c0"
   },
   "source": [
    "In this notebook, I would like to use the Hough transform to cluster hits. This notebook is therefore, get some materials from the past published kernels,\n",
    "\n",
    "In the previous notebook, we see a function relating phi and r like (where $r = \\sqrt{x^2 + y^2}$, $\\phi = arctan2(y/x)$):\n",
    "$$ \\phi_{new} = \\phi + i(ar + br^2),$$\n",
    "where $i$ is increased incrementally from 0 (straight tracks) to some number (curve tracks).\n",
    "\n",
    "\n",
    "However, the above equation is not exact to relate those two features. Instead, one might want to use the Hough transform:\n",
    "$$  \\frac{r}{2r_0} =  \\cos(\\phi - \\theta) $$\n",
    "\n",
    "In the above equation, $\\phi$ and $r$ are the original $\\phi$ and $r$ of each hit, while $r_0$ and $\\theta$ are the $r$ and $\\phi$ of a specific point in the XY plane, that is the origin of a circle in XY plane. That circle passes through the inspected hit. \n",
    "\n",
    "Then, our clustering problem can be stated this way:\n",
    "- For each $\\frac{1}{2r_0}$, starting from 0 (corresponding to straight tracks), to an appropriate stopping point, we calculate $\\theta = \\phi - \\arccos(\\frac{r}{2r_0})$\n",
    "- Group all hits with the near $\\theta$ and some other features to a detected track by DBSCAN. Since $\\theta$ can take very large or small values, using $\\sin(\\theta)$ and $\\cos(\\theta)$ is better.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "_cell_guid": "e081740e-8169-4481-b1df-f5dd5488314f",
    "_uuid": "0bee86255243664f24e4bcf48af2228a3100a8b7"
   },
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#import hdbscan\n",
    "from scipy import stats\n",
    "from tqdm import tqdm\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from trackml_helper import *\n",
    "from analysis import *\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "_cell_guid": "e06d1ed7-5091-4d67-abb4-5984b137e2e6",
    "_uuid": "c2f70ae63abffcc09a534bb17fb89df8ffddb722",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def merge(cl1, cl2): # merge cluster 2 to cluster 1\n",
    "    d = pd.DataFrame(data={'s1':cl1,'s2':cl2})\n",
    "    d['N1'] = d.groupby('s1')['s1'].transform('count')\n",
    "    d['N2'] = d.groupby('s2')['s2'].transform('count')\n",
    "    maxs1 = d['s1'].max()\n",
    "    cond = np.where((d['N2'].values>d['N1'].values) & (d['N2'].values<19)) # Locate the hit with the new cluster> old cluster\n",
    "    s1 = d['s1'].values \n",
    "    s1[cond] = d['s2'].values[cond]+maxs1 # Assign all hits that belong to the new track (+ maxs1 to increase the label for the track so it's different from the original).\n",
    "    return s1\n",
    "\n",
    "def extract_good_hits(truth, submission):\n",
    "    tru = truth[['hit_id', 'particle_id', 'weight']].merge(submission, how='left', on='hit_id')\n",
    "    tru['count_both'] = tru.groupby(['track_id', 'particle_id']).hit_id.transform('count')    \n",
    "    tru['count_particle'] = tru.groupby(['particle_id']).hit_id.transform('count')\n",
    "    tru['count_track'] = tru.groupby(['track_id']).hit_id.transform('count')\n",
    "    return tru[(tru.count_both > 0.5*tru.count_particle) & (tru.count_both > 0.5*tru.count_track)]\n",
    "\n",
    "def fast_score(good_hits_df):\n",
    "    return good_hits_df.weight.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True: #tuning\n",
    "    path_to_train = \"/home/alexanderliao/data/Kaggle/competitions/trackml-particle-identification/train\"\n",
    "    event_prefix = \"event000001000\"\n",
    "    hits, cells, particles, truth = load_event(os.path.join(path_to_train, event_prefix))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def analyze_truth_perspective(truth, submission):\n",
    "    tru = truth[['hit_id', 'particle_id', 'weight']].merge(submission, how='left', on='hit_id')\n",
    "    tru['count_both'] = tru.groupby(['track_id', 'particle_id']).hit_id.transform('count')    \n",
    "    tru['count_particle'] = tru.groupby(['particle_id']).hit_id.transform('count')\n",
    "    tru['count_track'] = tru.groupby(['track_id']).hit_id.transform('count')\n",
    "    good_hits = tru[(tru.count_both > 0.5*tru.count_particle) & (tru.count_both > 0.5*tru.count_track)]\n",
    "    score = good_hits.weight.sum()\n",
    "    \n",
    "    anatru = tru.particle_id.value_counts().value_counts().sort_index().to_frame().rename({'particle_id':'true_particle_counts'}, axis=1)\n",
    "    #anatru['true_particle_ratio'] = anatru['true_particle_counts'].values*100/np.sum(anatru['true_particle_counts'])\n",
    "\n",
    "    anatru['good_tracks_counts'] = np.zeros(len(anatru)).astype(int)\n",
    "    anatru['good_tracks_intersect_nhits_avg'] = np.zeros(len(anatru))\n",
    "    anatru['best_detect_intersect_nhits_avg'] = np.zeros(len(anatru))\n",
    "    for nhit in tqdm(range(4,20)):\n",
    "        particle_list  = tru[(tru.count_particle==nhit)].particle_id.unique()\n",
    "        intersect_count = 0\n",
    "        good_tracks_count = 0\n",
    "        good_tracks_intersect = 0\n",
    "        for p in particle_list:\n",
    "            nhit_intersect = tru[tru.particle_id==p].count_both.max()\n",
    "            intersect_count += nhit_intersect\n",
    "            corresponding_track = tru.loc[tru[tru.particle_id==p].count_both.idxmax()].track_id\n",
    "            leng_corresponding_track = len(tru[tru.track_id == corresponding_track])\n",
    "            \n",
    "            if (nhit_intersect >= nhit/2) and (nhit_intersect >= leng_corresponding_track/2):\n",
    "                good_tracks_count += 1\n",
    "                good_tracks_intersect += nhit_intersect\n",
    "        intersect_count = intersect_count/len(particle_list)\n",
    "        anatru.at[nhit,'best_detect_intersect_nhits_avg'] = intersect_count\n",
    "        anatru.at[nhit,'good_tracks_counts'] = good_tracks_count\n",
    "        if good_tracks_count > 0:\n",
    "            anatru.at[nhit,'good_tracks_intersect_nhits_avg'] = good_tracks_intersect/good_tracks_count\n",
    "    \n",
    "    return score, anatru, good_hits\n",
    "\n",
    "def precision(truth, submission,min_hits):\n",
    "    tru = truth[['hit_id', 'particle_id', 'weight']].merge(submission, how='left', on='hit_id')\n",
    "    tru['count_both'] = tru.groupby(['track_id', 'particle_id']).hit_id.transform('count')    \n",
    "    tru['count_particle'] = tru.groupby(['particle_id']).hit_id.transform('count')\n",
    "    tru['count_track'] = tru.groupby(['track_id']).hit_id.transform('count')\n",
    "    #print('Analyzing predictions...')\n",
    "    predicted_list  = tru[(tru.count_track>=min_hits)].track_id.unique()\n",
    "    good_tracks_count = 0\n",
    "    ghost_tracks_count = 0\n",
    "    fp_weights = 0\n",
    "    tp_weights = 0\n",
    "    for t in predicted_list:\n",
    "        nhit_track = tru[tru.track_id==t].count_track.iloc[0]\n",
    "        nhit_intersect = tru[tru.track_id==t].count_both.max()\n",
    "        corresponding_particle = tru.loc[tru[tru.track_id==t].count_both.idxmax()].particle_id\n",
    "        leng_corresponding_particle = len(tru[tru.particle_id == corresponding_particle])\n",
    "        if (nhit_intersect >= nhit_track/2) and (nhit_intersect >= leng_corresponding_particle/2): #if the predicted track is good\n",
    "            good_tracks_count += 1\n",
    "            tp_weights += tru[(tru.track_id==t)&(tru.particle_id==corresponding_particle)].weight.sum()\n",
    "            fp_weights += tru[(tru.track_id==t)&(tru.particle_id!=corresponding_particle)].weight.sum()\n",
    "        else: # if the predicted track is bad\n",
    "                ghost_tracks_count += 1\n",
    "                fp_weights += tru[(tru.track_id==t)].weight.sum()\n",
    "    all_weights = tru[(tru.count_track>=min_hits)].weight.sum()\n",
    "    precision = tp_weights/all_weights*100\n",
    "    print('Precision: ',precision,', good tracks:', good_tracks_count,', total tracks:',len(predicted_list),\n",
    "           ', loss:', fp_weights, ', reco:', tp_weights, 'reco/loss', tp_weights/fp_weights)\n",
    "    return precision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_one_event_submission(event_id, hits, labels):\n",
    "    sub_data = np.column_stack(([event_id]*len(hits), hits.hit_id.values, labels))\n",
    "    submission = pd.DataFrame(data=sub_data, columns=[\"event_id\", \"hit_id\", \"track_id\"]).astype(int)\n",
    "    return submission  \n",
    "\n",
    "def preprocess_hits(h,dz):\n",
    "    h['z'] =  h['z'].values + dz\n",
    "    h['r'] = np.sqrt(h['x'].values**2+h['y'].values**2+h['z'].values**2)\n",
    "    h['rt'] = np.sqrt(h['x'].values**2+h['y'].values**2)\n",
    "    h['a0'] = np.arctan2(h['y'].values,h['x'].values)\n",
    "    h['zdivrt'] = h['z'].values/h['rt'].values\n",
    "    h['zdivr'] = h['z'].values/h['r'].values\n",
    "    h['xdivr'] = h['x'].values / h['r'].values\n",
    "    h['ydivr'] = h['y'].values / h['r'].values\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smart_arccos(x):\n",
    "    max_mask = x > 1\n",
    "    min_mask = x < -1\n",
    "    ret = np.arccos(x, where=~(max_mask|min_mask))\n",
    "    ret[max_mask] = 0.0\n",
    "    ret[min_mask] = np.pi\n",
    "    return np.pi\n",
    "\n",
    "class Clusterer(object):\n",
    "    def __init__(self):                        \n",
    "        self.abc = []\n",
    "          \n",
    "    def initialize(self,dfhits):\n",
    "        self.cluster = range(len(dfhits))\n",
    "        \n",
    "    def Hough_clustering(self,dfh,coef,epsilon,min_samples=1,n_loop=180,verbose=True): # [phi_coef,phi_coef,zdivrt_coef,zdivr_coef,xdivr_coef,ydivr_coef]\n",
    "        merged_cluster = self.cluster\n",
    "        mm = 1\n",
    "        stepii = 0.000005\n",
    "        count_ii = 0\n",
    "        adaptive_eps_coefficient = 1\n",
    "        #z = np.arange(-5.5,5.5,0.01)\n",
    "        #random.choice(z)\n",
    "        for ii in np.arange(0, n_loop*stepii, stepii):\n",
    "            count_ii += 1\n",
    "            for jj in range(2):\n",
    "                mm = mm*(-1)\n",
    "                eps_new = epsilon + count_ii*adaptive_eps_coefficient*10**(-5)\n",
    "                #eps_new = 0.0035\n",
    "                \n",
    "                \"\"\"\n",
    "                dfh['a1'] = dfh['a0'].values - smart_arccos(mm*ii*dfh['rt'].values)\n",
    "                dfh['sina1']= np.sin(dfh['a1'].values)\n",
    "                dfh['cosa1']= np.cos(dfh['a1'].values)\n",
    "                \n",
    "                \"\"\"\n",
    "                dfh['a1'] = dfh['a0'].values - np.arccos(mm*ii*dfh['rt'].values)\n",
    "                cond=np.where(np.isfinite(dfh['a1'].values))\n",
    "                dfh['sina1'] = np.random.rand(len(dfh))\n",
    "                dfh['cosa1'] = np.random.rand(len(dfh))\n",
    "                dfh['sina1'].values[cond] = np.sin(dfh['a1'].values[cond])\n",
    "                dfh['cosa1'].values[cond] = np.cos(dfh['a1'].values[cond])\n",
    "                \n",
    "                \n",
    "                ss = StandardScaler()\n",
    "                dfs = ss.fit_transform(dfh[['sina1','cosa1','zdivrt','zdivr','xdivr','ydivr']].values) \n",
    "                #dfs = scale_ignore_nan(dfh[['sina1','cosa1','zdivrt','zdivr','xdivr','ydivr']])\n",
    "                dfs = np.multiply(dfs, coef)\n",
    "                new_cluster=DBSCAN(eps=eps_new,min_samples=min_samples,metric='euclidean',n_jobs=-1).fit(dfs).labels_\n",
    "                \n",
    "                cond=np.where(np.bincount(new_cluster)>45)\n",
    "                new_cluster[cond] = np.random.randint(low=max(new_cluster),size=len(cond))\n",
    "                \n",
    "                merged_cluster = merge(merged_cluster, new_cluster)\n",
    "                \n",
    "                if verbose == True:\n",
    "                    sub = create_one_event_submission(0, hits, merged_cluster)\n",
    "                    good_hits = extract_good_hits(truth, sub)\n",
    "                    score_1 = fast_score(good_hits)\n",
    "                    print('2r0_inverse:', ii*mm ,'. Score:', score_1)\n",
    "                    #clear_output(wait=True)\n",
    "        self.cluster = merged_cluster                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2r0_inverse: -0.0 . Score: 0.053846158\n",
      "2r0_inverse: 0.0 . Score: 0.053846158\n",
      "2r0_inverse: -5e-06 . Score: 0.05451239\n",
      "2r0_inverse: 5e-06 . Score: 0.05676371\n",
      "2r0_inverse: -1e-05 . Score: 0.057830937\n",
      "2r0_inverse: 1e-05 . Score: 0.058984976\n",
      "2r0_inverse: -1.5000000000000002e-05 . Score: 0.059996903\n",
      "2r0_inverse: 1.5000000000000002e-05 . Score: 0.06247089\n",
      "2r0_inverse: -2e-05 . Score: 0.06359076\n",
      "2r0_inverse: 2e-05 . Score: 0.064547956\n",
      "2r0_inverse: -2.5e-05 . Score: 0.06550863\n",
      "2r0_inverse: 2.5e-05 . Score: 0.06663377\n",
      "2r0_inverse: -3.0000000000000004e-05 . Score: 0.06751122\n",
      "2r0_inverse: 3.0000000000000004e-05 . Score: 0.068532884\n",
      "2r0_inverse: -3.5000000000000004e-05 . Score: 0.06881915\n",
      "2r0_inverse: 3.5000000000000004e-05 . Score: 0.071201414\n",
      "2r0_inverse: -4e-05 . Score: 0.07284404\n",
      "2r0_inverse: 4e-05 . Score: 0.07483402\n",
      "2r0_inverse: -4.5e-05 . Score: 0.07622239\n",
      "2r0_inverse: 4.5e-05 . Score: 0.07846488\n",
      "2r0_inverse: -5e-05 . Score: 0.080625296\n",
      "2r0_inverse: 5e-05 . Score: 0.08253196\n",
      "2r0_inverse: -5.5e-05 . Score: 0.08507048\n",
      "2r0_inverse: 5.5e-05 . Score: 0.086744905\n",
      "2r0_inverse: -6.000000000000001e-05 . Score: 0.08792175\n",
      "2r0_inverse: 6.000000000000001e-05 . Score: 0.090229675\n",
      "2r0_inverse: -6.500000000000001e-05 . Score: 0.09295777\n",
      "2r0_inverse: 6.500000000000001e-05 . Score: 0.09465598\n",
      "2r0_inverse: -7.000000000000001e-05 . Score: 0.095612735\n",
      "2r0_inverse: 7.000000000000001e-05 . Score: 0.09791801\n",
      "2r0_inverse: -7.500000000000001e-05 . Score: 0.09980267\n",
      "2r0_inverse: 7.500000000000001e-05 . Score: 0.10230298\n",
      "2r0_inverse: -8e-05 . Score: 0.103909746\n",
      "2r0_inverse: 8e-05 . Score: 0.10656505\n",
      "2r0_inverse: -8.5e-05 . Score: 0.10876575\n",
      "2r0_inverse: 8.5e-05 . Score: 0.11096945\n",
      "2r0_inverse: -9e-05 . Score: 0.113680705\n",
      "2r0_inverse: 9e-05 . Score: 0.11808305\n",
      "2r0_inverse: -9.5e-05 . Score: 0.119770706\n",
      "2r0_inverse: 9.5e-05 . Score: 0.122420296\n",
      "2r0_inverse: -0.0001 . Score: 0.1245328\n",
      "2r0_inverse: 0.0001 . Score: 0.1262973\n",
      "2r0_inverse: -0.000105 . Score: 0.12787524\n",
      "2r0_inverse: 0.000105 . Score: 0.12974556\n",
      "2r0_inverse: -0.00011 . Score: 0.13180228\n",
      "2r0_inverse: 0.00011 . Score: 0.13439894\n",
      "2r0_inverse: -0.000115 . Score: 0.1362423\n",
      "2r0_inverse: 0.000115 . Score: 0.14017357\n",
      "2r0_inverse: -0.00012000000000000002 . Score: 0.14266875\n",
      "2r0_inverse: 0.00012000000000000002 . Score: 0.14475684\n",
      "2r0_inverse: -0.000125 . Score: 0.14743981\n",
      "2r0_inverse: 0.000125 . Score: 0.15067169\n",
      "2r0_inverse: -0.00013000000000000002 . Score: 0.15353237\n",
      "2r0_inverse: 0.00013000000000000002 . Score: 0.15598701\n",
      "2r0_inverse: -0.000135 . Score: 0.15749413\n",
      "2r0_inverse: 0.000135 . Score: 0.16033523\n",
      "2r0_inverse: -0.00014000000000000001 . Score: 0.16316424\n",
      "2r0_inverse: 0.00014000000000000001 . Score: 0.16684027\n",
      "2r0_inverse: -0.000145 . Score: 0.16819903\n",
      "2r0_inverse: 0.000145 . Score: 0.17155167\n",
      "2r0_inverse: -0.00015000000000000001 . Score: 0.17453083\n",
      "2r0_inverse: 0.00015000000000000001 . Score: 0.17714311\n",
      "2r0_inverse: -0.000155 . Score: 0.17998259\n",
      "2r0_inverse: 0.000155 . Score: 0.18159029\n",
      "2r0_inverse: -0.00016 . Score: 0.18481338\n",
      "2r0_inverse: 0.00016 . Score: 0.18817139\n",
      "2r0_inverse: -0.00016500000000000003 . Score: 0.19088486\n",
      "2r0_inverse: 0.00016500000000000003 . Score: 0.19393724\n",
      "2r0_inverse: -0.00017 . Score: 0.19609389\n",
      "2r0_inverse: 0.00017 . Score: 0.19766773\n",
      "2r0_inverse: -0.00017500000000000003 . Score: 0.19985372\n",
      "2r0_inverse: 0.00017500000000000003 . Score: 0.20283434\n",
      "2r0_inverse: -0.00018 . Score: 0.20465757\n",
      "2r0_inverse: 0.00018 . Score: 0.20794198\n",
      "2r0_inverse: -0.00018500000000000002 . Score: 0.21006796\n",
      "2r0_inverse: 0.00018500000000000002 . Score: 0.21253443\n",
      "2r0_inverse: -0.00019 . Score: 0.21366018\n",
      "2r0_inverse: 0.00019 . Score: 0.21631378\n",
      "2r0_inverse: -0.00019500000000000002 . Score: 0.21828306\n",
      "2r0_inverse: 0.00019500000000000002 . Score: 0.22041966\n",
      "2r0_inverse: -0.0002 . Score: 0.22178751\n",
      "2r0_inverse: 0.0002 . Score: 0.22436886\n",
      "2r0_inverse: -0.00020500000000000002 . Score: 0.22672369\n",
      "2r0_inverse: 0.00020500000000000002 . Score: 0.22871533\n",
      "2r0_inverse: -0.00021 . Score: 0.23158248\n",
      "2r0_inverse: 0.00021 . Score: 0.23437488\n",
      "2r0_inverse: -0.00021500000000000002 . Score: 0.2372187\n",
      "2r0_inverse: 0.00021500000000000002 . Score: 0.23865293\n",
      "2r0_inverse: -0.00022 . Score: 0.24061035\n",
      "2r0_inverse: 0.00022 . Score: 0.24389017\n",
      "2r0_inverse: -0.00022500000000000002 . Score: 0.24643458\n",
      "2r0_inverse: 0.00022500000000000002 . Score: 0.24869694\n",
      "2r0_inverse: -0.00023 . Score: 0.25013283\n",
      "2r0_inverse: 0.00023 . Score: 0.25214845\n",
      "2r0_inverse: -0.00023500000000000002 . Score: 0.25439212\n",
      "2r0_inverse: 0.00023500000000000002 . Score: 0.25630638\n",
      "2r0_inverse: -0.00024000000000000003 . Score: 0.25825456\n",
      "2r0_inverse: 0.00024000000000000003 . Score: 0.26054618\n",
      "2r0_inverse: -0.00024500000000000005 . Score: 0.2621315\n",
      "2r0_inverse: 0.00024500000000000005 . Score: 0.26440814\n",
      "2r0_inverse: -0.00025 . Score: 0.2662102\n",
      "2r0_inverse: 0.00025 . Score: 0.2680346\n",
      "2r0_inverse: -0.000255 . Score: 0.27007818\n",
      "2r0_inverse: 0.000255 . Score: 0.2719982\n",
      "2r0_inverse: -0.00026000000000000003 . Score: 0.27460167\n",
      "2r0_inverse: 0.00026000000000000003 . Score: 0.27589855\n",
      "2r0_inverse: -0.00026500000000000004 . Score: 0.27759323\n",
      "2r0_inverse: 0.00026500000000000004 . Score: 0.27953702\n",
      "2r0_inverse: -0.00027 . Score: 0.28094044\n",
      "2r0_inverse: 0.00027 . Score: 0.28285524\n",
      "2r0_inverse: -0.000275 . Score: 0.28548354\n",
      "2r0_inverse: 0.000275 . Score: 0.28710014\n",
      "2r0_inverse: -0.00028000000000000003 . Score: 0.28853738\n",
      "2r0_inverse: 0.00028000000000000003 . Score: 0.29085362\n",
      "2r0_inverse: -0.00028500000000000004 . Score: 0.29229584\n",
      "2r0_inverse: 0.00028500000000000004 . Score: 0.29446357\n",
      "2r0_inverse: -0.00029 . Score: 0.296226\n",
      "2r0_inverse: 0.00029 . Score: 0.29779592\n",
      "2r0_inverse: -0.000295 . Score: 0.3002609\n",
      "2r0_inverse: 0.000295 . Score: 0.30177152\n",
      "2r0_inverse: -0.00030000000000000003 . Score: 0.3036423\n",
      "2r0_inverse: 0.00030000000000000003 . Score: 0.30629244\n",
      "2r0_inverse: -0.00030500000000000004 . Score: 0.30877537\n",
      "2r0_inverse: 0.00030500000000000004 . Score: 0.3106507\n",
      "2r0_inverse: -0.00031 . Score: 0.3120891\n",
      "2r0_inverse: 0.00031 . Score: 0.31360865\n",
      "2r0_inverse: -0.000315 . Score: 0.3157957\n",
      "2r0_inverse: 0.000315 . Score: 0.3171687\n",
      "2r0_inverse: -0.00032 . Score: 0.31905773\n",
      "2r0_inverse: 0.00032 . Score: 0.32033753\n",
      "2r0_inverse: -0.00032500000000000004 . Score: 0.3217649\n",
      "2r0_inverse: 0.00032500000000000004 . Score: 0.32335228\n",
      "2r0_inverse: -0.00033000000000000005 . Score: 0.324848\n",
      "2r0_inverse: 0.00033000000000000005 . Score: 0.32696664\n",
      "2r0_inverse: -0.000335 . Score: 0.3284998\n",
      "2r0_inverse: 0.000335 . Score: 0.33087182\n",
      "2r0_inverse: -0.00034 . Score: 0.3322098\n",
      "2r0_inverse: 0.00034 . Score: 0.3340091\n",
      "2r0_inverse: -0.00034500000000000004 . Score: 0.3357335\n",
      "2r0_inverse: 0.00034500000000000004 . Score: 0.33682787\n",
      "2r0_inverse: -0.00035000000000000005 . Score: 0.33875474\n",
      "2r0_inverse: 0.00035000000000000005 . Score: 0.3399622\n",
      "2r0_inverse: -0.000355 . Score: 0.34180552\n",
      "2r0_inverse: 0.000355 . Score: 0.3434236\n",
      "2r0_inverse: -0.00036 . Score: 0.3448942\n",
      "2r0_inverse: 0.00036 . Score: 0.34639204\n",
      "2r0_inverse: -0.00036500000000000004 . Score: 0.3476965\n",
      "2r0_inverse: 0.00036500000000000004 . Score: 0.3489675\n",
      "2r0_inverse: -0.00037000000000000005 . Score: 0.35047486\n",
      "2r0_inverse: 0.00037000000000000005 . Score: 0.35155618\n",
      "2r0_inverse: -0.000375 . Score: 0.3527186\n",
      "2r0_inverse: 0.000375 . Score: 0.35409504\n",
      "2r0_inverse: -0.00038 . Score: 0.3554826\n",
      "2r0_inverse: 0.00038 . Score: 0.35660505\n",
      "2r0_inverse: -0.00038500000000000003 . Score: 0.3583554\n",
      "2r0_inverse: 0.00038500000000000003 . Score: 0.36003613\n",
      "2r0_inverse: -0.00039000000000000005 . Score: 0.3615605\n",
      "2r0_inverse: 0.00039000000000000005 . Score: 0.36304694\n",
      "2r0_inverse: -0.000395 . Score: 0.36425945\n",
      "2r0_inverse: 0.000395 . Score: 0.36561486\n",
      "2r0_inverse: -0.0004 . Score: 0.36701447\n",
      "2r0_inverse: 0.0004 . Score: 0.3687245\n",
      "2r0_inverse: -0.00040500000000000003 . Score: 0.3702281\n",
      "2r0_inverse: 0.00040500000000000003 . Score: 0.3714792\n",
      "2r0_inverse: -0.00041000000000000005 . Score: 0.373168\n",
      "2r0_inverse: 0.00041000000000000005 . Score: 0.37455177\n",
      "2r0_inverse: -0.00041500000000000006 . Score: 0.37603956\n",
      "2r0_inverse: 0.00041500000000000006 . Score: 0.37734112\n",
      "2r0_inverse: -0.00042 . Score: 0.37876743\n",
      "2r0_inverse: 0.00042 . Score: 0.3804784\n",
      "2r0_inverse: -0.00042500000000000003 . Score: 0.38123757\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2r0_inverse: 0.00042500000000000003 . Score: 0.38240448\n",
      "2r0_inverse: -0.00043000000000000004 . Score: 0.383628\n",
      "2r0_inverse: 0.00043000000000000004 . Score: 0.38502222\n",
      "2r0_inverse: -0.00043500000000000006 . Score: 0.38558334\n",
      "2r0_inverse: 0.00043500000000000006 . Score: 0.3868919\n",
      "2r0_inverse: -0.00044 . Score: 0.3878975\n",
      "2r0_inverse: 0.00044 . Score: 0.38943583\n",
      "2r0_inverse: -0.00044500000000000003 . Score: 0.39079583\n",
      "2r0_inverse: 0.00044500000000000003 . Score: 0.3924527\n",
      "2r0_inverse: -0.00045000000000000004 . Score: 0.39375347\n",
      "2r0_inverse: 0.00045000000000000004 . Score: 0.39487442\n",
      "2r0_inverse: -0.00045500000000000006 . Score: 0.39579976\n",
      "2r0_inverse: 0.00045500000000000006 . Score: 0.39666358\n",
      "2r0_inverse: -0.00046 . Score: 0.3980684\n",
      "2r0_inverse: 0.00046 . Score: 0.39935327\n",
      "2r0_inverse: -0.000465 . Score: 0.4003364\n",
      "2r0_inverse: 0.000465 . Score: 0.40213943\n",
      "2r0_inverse: -0.00047000000000000004 . Score: 0.4029681\n",
      "2r0_inverse: 0.00047000000000000004 . Score: 0.404145\n",
      "2r0_inverse: -0.00047500000000000005 . Score: 0.40516037\n",
      "2r0_inverse: 0.00047500000000000005 . Score: 0.40617633\n",
      "2r0_inverse: -0.00048000000000000007 . Score: 0.4076447\n",
      "2r0_inverse: 0.00048000000000000007 . Score: 0.40897983\n",
      "2r0_inverse: -0.000485 . Score: 0.4099869\n",
      "2r0_inverse: 0.000485 . Score: 0.411783\n",
      "2r0_inverse: -0.0004900000000000001 . Score: 0.41289437\n",
      "2r0_inverse: 0.0004900000000000001 . Score: 0.41356683\n",
      "2r0_inverse: -0.000495 . Score: 0.41408867\n",
      "2r0_inverse: 0.000495 . Score: 0.41590935\n",
      "2r0_inverse: -0.0005 . Score: 0.41708612\n",
      "2r0_inverse: 0.0005 . Score: 0.4179272\n",
      "2r0_inverse: -0.000505 . Score: 0.41903526\n",
      "2r0_inverse: 0.000505 . Score: 0.42010427\n",
      "2r0_inverse: -0.00051 . Score: 0.4214599\n",
      "2r0_inverse: 0.00051 . Score: 0.42221344\n",
      "2r0_inverse: -0.000515 . Score: 0.42315486\n",
      "2r0_inverse: 0.000515 . Score: 0.4241221\n",
      "2r0_inverse: -0.0005200000000000001 . Score: 0.42523888\n",
      "2r0_inverse: 0.0005200000000000001 . Score: 0.42653242\n",
      "2r0_inverse: -0.0005250000000000001 . Score: 0.4277684\n",
      "2r0_inverse: 0.0005250000000000001 . Score: 0.428756\n",
      "2r0_inverse: -0.0005300000000000001 . Score: 0.42972293\n",
      "2r0_inverse: 0.0005300000000000001 . Score: 0.4311353\n",
      "2r0_inverse: -0.000535 . Score: 0.43215317\n",
      "2r0_inverse: 0.000535 . Score: 0.43289664\n",
      "2r0_inverse: -0.00054 . Score: 0.43366614\n",
      "2r0_inverse: 0.00054 . Score: 0.43447298\n",
      "2r0_inverse: -0.000545 . Score: 0.43500948\n",
      "2r0_inverse: 0.000545 . Score: 0.43588513\n",
      "2r0_inverse: -0.00055 . Score: 0.4364606\n",
      "2r0_inverse: 0.00055 . Score: 0.43710113\n",
      "2r0_inverse: -0.000555 . Score: 0.43758667\n",
      "2r0_inverse: 0.000555 . Score: 0.43842903\n",
      "2r0_inverse: -0.0005600000000000001 . Score: 0.4392832\n",
      "2r0_inverse: 0.0005600000000000001 . Score: 0.43976855\n",
      "2r0_inverse: -0.0005650000000000001 . Score: 0.44024894\n",
      "2r0_inverse: 0.0005650000000000001 . Score: 0.44119328\n",
      "2r0_inverse: -0.0005700000000000001 . Score: 0.44148794\n",
      "2r0_inverse: 0.0005700000000000001 . Score: 0.44233266\n",
      "2r0_inverse: -0.0005750000000000001 . Score: 0.44297045\n",
      "2r0_inverse: 0.0005750000000000001 . Score: 0.4437356\n",
      "2r0_inverse: -0.00058 . Score: 0.44437647\n",
      "2r0_inverse: 0.00058 . Score: 0.4452526\n",
      "2r0_inverse: -0.000585 . Score: 0.44610715\n",
      "2r0_inverse: 0.000585 . Score: 0.44656387\n",
      "2r0_inverse: -0.00059 . Score: 0.4472213\n",
      "2r0_inverse: 0.00059 . Score: 0.4479848\n",
      "2r0_inverse: -0.000595 . Score: 0.44865447\n",
      "2r0_inverse: 0.000595 . Score: 0.4491452\n",
      "2r0_inverse: -0.0006000000000000001 . Score: 0.4502245\n",
      "2r0_inverse: 0.0006000000000000001 . Score: 0.45082158\n",
      "2r0_inverse: -0.0006050000000000001 . Score: 0.45107165\n",
      "2r0_inverse: 0.0006050000000000001 . Score: 0.45188156\n",
      "2r0_inverse: -0.0006100000000000001 . Score: 0.45263824\n",
      "2r0_inverse: 0.0006100000000000001 . Score: 0.45333087\n",
      "2r0_inverse: -0.0006150000000000001 . Score: 0.45405632\n",
      "2r0_inverse: 0.0006150000000000001 . Score: 0.45461005\n",
      "2r0_inverse: -0.00062 . Score: 0.45525065\n",
      "2r0_inverse: 0.00062 . Score: 0.45538586\n",
      "2r0_inverse: -0.000625 . Score: 0.4563012\n",
      "2r0_inverse: 0.000625 . Score: 0.45647076\n",
      "2r0_inverse: -0.00063 . Score: 0.45725006\n",
      "2r0_inverse: 0.00063 . Score: 0.4578323\n",
      "2r0_inverse: -0.000635 . Score: 0.4583187\n",
      "2r0_inverse: 0.000635 . Score: 0.45897228\n",
      "2r0_inverse: -0.00064 . Score: 0.45963672\n",
      "2r0_inverse: 0.00064 . Score: 0.46031663\n",
      "2r0_inverse: -0.0006450000000000001 . Score: 0.46089998\n",
      "2r0_inverse: 0.0006450000000000001 . Score: 0.4617561\n",
      "2r0_inverse: -0.0006500000000000001 . Score: 0.4625462\n",
      "2r0_inverse: 0.0006500000000000001 . Score: 0.4632481\n",
      "2r0_inverse: -0.0006550000000000001 . Score: 0.46386704\n",
      "2r0_inverse: 0.0006550000000000001 . Score: 0.4640835\n",
      "2r0_inverse: -0.0006600000000000001 . Score: 0.46436268\n",
      "2r0_inverse: 0.0006600000000000001 . Score: 0.4647344\n",
      "2r0_inverse: -0.000665 . Score: 0.4652102\n",
      "2r0_inverse: 0.000665 . Score: 0.465911\n",
      "2r0_inverse: -0.00067 . Score: 0.46636617\n",
      "2r0_inverse: 0.00067 . Score: 0.46692932\n",
      "2r0_inverse: -0.000675 . Score: 0.46760404\n",
      "2r0_inverse: 0.000675 . Score: 0.46792445\n",
      "2r0_inverse: -0.00068 . Score: 0.46839628\n",
      "2r0_inverse: 0.00068 . Score: 0.468913\n",
      "2r0_inverse: -0.0006850000000000001 . Score: 0.46928573\n",
      "2r0_inverse: 0.0006850000000000001 . Score: 0.4703178\n",
      "2r0_inverse: -0.0006900000000000001 . Score: 0.47069308\n",
      "2r0_inverse: 0.0006900000000000001 . Score: 0.47117007\n",
      "2r0_inverse: -0.0006950000000000001 . Score: 0.47168845\n",
      "2r0_inverse: 0.0006950000000000001 . Score: 0.4722215\n",
      "2r0_inverse: -0.0007000000000000001 . Score: 0.47267357\n",
      "2r0_inverse: 0.0007000000000000001 . Score: 0.47302142\n",
      "2r0_inverse: -0.000705 . Score: 0.47330174\n",
      "2r0_inverse: 0.000705 . Score: 0.47417325\n",
      "2r0_inverse: -0.00071 . Score: 0.47453326\n",
      "2r0_inverse: 0.00071 . Score: 0.47490874\n",
      "2r0_inverse: -0.000715 . Score: 0.47544074\n",
      "2r0_inverse: 0.000715 . Score: 0.4759822\n",
      "2r0_inverse: -0.00072 . Score: 0.47648168\n",
      "2r0_inverse: 0.00072 . Score: 0.47689855\n",
      "2r0_inverse: -0.0007250000000000001 . Score: 0.47756153\n",
      "2r0_inverse: 0.0007250000000000001 . Score: 0.47789514\n",
      "2r0_inverse: -0.0007300000000000001 . Score: 0.47823712\n",
      "2r0_inverse: 0.0007300000000000001 . Score: 0.47877938\n",
      "2r0_inverse: -0.0007350000000000001 . Score: 0.47917697\n",
      "2r0_inverse: 0.0007350000000000001 . Score: 0.47974044\n",
      "2r0_inverse: -0.0007400000000000001 . Score: 0.47995058\n",
      "2r0_inverse: 0.0007400000000000001 . Score: 0.48016793\n"
     ]
    }
   ],
   "source": [
    "# Clustering by varying \n",
    "#model = Clusterer()\n",
    "#model.initialize(hits) \n",
    "c = [1.5,1.5,0.73,0.17,0.027,0.027] #[phi_coef,phi_coef,zdivrt_coef,zdivr_coef,xdivr_coef,ydivr_coef]\n",
    "min_samples_in_cluster = 1\n",
    "\n",
    "model = Clusterer()\n",
    "model.initialize(hits) \n",
    "hits_with_dz = preprocess_hits(hits, 0)\n",
    "model.Hough_clustering(hits_with_dz,coef=c,epsilon=0.0048,min_samples=min_samples_in_cluster,\n",
    "                       n_loop=300,verbose=True)\n",
    "\n",
    "submission = create_one_event_submission(0, hits, model.cluster)\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr = precision(truth,submission,min_hits=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr = precision(truth,submission,min_hits=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr = precision(truth,submission,min_hits=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr = precision(truth,submission,min_hits=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False: #benchmark\n",
    "    c = [1.6,1.6,0.73,0.17,0.027,0.027]\n",
    "    model = Clusterer()\n",
    "    model.initialize(hits) \n",
    "    hits_with_dz = preprocess_hits(hits, 0)\n",
    "    model.Clustering(hits,coef=c,\n",
    "                           epsilon=0.0048,\n",
    "                           min_samples=1,\n",
    "                           n_loop=180,\n",
    "                           stepii=0.000005,\n",
    "                           verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering by varying \n",
    "#model = Clusterer()\n",
    "#model.initialize(hits) \n",
    "\n",
    "# Preparing Submission\n",
    "if False:\n",
    "    for i in tqdm(range(62,125)):\n",
    "        path_to_train = \"/home/alexanderliao/data/Kaggle/competitions/trackml-particle-identification/test\"\n",
    "        event_prefix = \"event\"+str(i).zfill(9)\n",
    "        hits = load_event_hits(os.path.join(path_to_train, event_prefix))\n",
    "        c = [1.6,1.6,0.73,0.17,0.027,0.027] #[phi_coef,phi_coef,zdivrt_coef,zdivr_coef,xdivr_coef,ydivr_coef]\n",
    "        min_samples_in_cluster = 1\n",
    "\n",
    "        model = Clusterer()\n",
    "        model.initialize(hits) \n",
    "        hits_with_dz = preprocess_hits(hits, 0)\n",
    "        model.Hough_clustering(hits_with_dz,coef=c,epsilon=0.0048,min_samples=min_samples_in_cluster,\n",
    "                               n_loop=300,verbose=False)\n",
    "\n",
    "        if i == 62:\n",
    "            submission = create_one_event_submission(i, hits, model.cluster)\n",
    "        else:\n",
    "            submission = pd.concat([submission,create_one_event_submission(i, hits, model.cluster)])\n",
    "        print(submission)\n",
    "        if False: # O(n^2) if turned on\n",
    "            submission.to_csv('submission.csv')\n",
    "    print('\\n') \n",
    "    submission.to_csv('submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission\n",
    "df = submission.track_id.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "6967634/len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(63,124):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2df8584057590d42adef24a1286a92599f886df3"
   },
   "source": [
    "Now, let us see some analysis on the clustering result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d496bcbe21a377925f64c69de5a6c811c80d58a7"
   },
   "outputs": [],
   "source": [
    "pr = precision(truth,submission,min_hits=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fc582df7cf36f13b9ee0331572db305521e6336e"
   },
   "outputs": [],
   "source": [
    "pr = precision(truth,submission,min_hits=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8d22eb6747f01fa2af6bd2086cde122825e323a3"
   },
   "outputs": [],
   "source": [
    "pr = precision(truth,submission,min_hits=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "71aeb104416c23c2e57c45f7569939ff975780de"
   },
   "outputs": [],
   "source": [
    "pr = precision(truth,submission,min_hits=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "184b5b3541d8f8a82bdce122b3dc9b041af0d984"
   },
   "source": [
    "As one can see, long tracks have high precision, low lost weights. On the other hand, there are too many ghost short tracks. Then, we can use multi-stage clustering, using min_hits in DBSCAN for each stage (i.e., cluster long tracks first, then cluster short tracks with different parameters without touching the long tracks...)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "efd3174b9193b3180af1648d45d43db042e1fc55"
   },
   "source": [
    "Some other notes:\n",
    "\n",
    "+ Use too many loops can decrease the performance, as one can see from the log result above.\n",
    "\n",
    "+ No z-shifting is performed  (dz = 0), although the function preprocess already offer it. Some may want to use z-shifting right away just by change dz from 0 to any number between [-5.5, 5.5]\n",
    "\n",
    "+ Features are not optimized. Honestly, I am also stuck at searching for good features (and good weights). It would be very nice if someone secretly tell me those magic features :-).\n",
    "\n",
    "+ When $r/(2r_0) > 1$ or $< 1$, arccos is undefined, hence a warning appears (if running on local notebook). The problem, more importantly, is not about the warning. It is a technical issue: all hits with $r/(2r_0) > 1$ or $< 1$ MUST BE EXCLUDED from DBSCAN, because there will be NO track with that parameter pass through those hits. This can be done by some indexing techniques that I do not provide here. (DBSCAN uses a raw matrix to cluster, then we must be careful when exclude hits from the original full hit dataframe).\n",
    "\n",
    "KV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b8a951a9c4989186d8716a25e087fde36bcc30e2"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
