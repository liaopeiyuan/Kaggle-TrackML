{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d99098fcbfa6a9ec3ca72e4f681233c323c255c0"
   },
   "source": [
    "In this notebook, I would like to use the Hough transform to cluster hits. This notebook is therefore, get some materials from the past published kernels,\n",
    "\n",
    "In the previous notebook, we see a function relating phi and r like (where $r = \\sqrt{x^2 + y^2}$, $\\phi = arctan2(y/x)$):\n",
    "$$ \\phi_{new} = \\phi + i(ar + br^2),$$\n",
    "where $i$ is increased incrementally from 0 (straight tracks) to some number (curve tracks).\n",
    "\n",
    "\n",
    "However, the above equation is not exact to relate those two features. Instead, one might want to use the Hough transform:\n",
    "$$  \\frac{r}{2r_0} =  \\cos(\\phi - \\theta) $$\n",
    "\n",
    "In the above equation, $\\phi$ and $r$ are the original $\\phi$ and $r$ of each hit, while $r_0$ and $\\theta$ are the $r$ and $\\phi$ of a specific point in the XY plane, that is the origin of a circle in XY plane. That circle passes through the inspected hit. \n",
    "\n",
    "Then, our clustering problem can be stated this way:\n",
    "- For each $\\frac{1}{2r_0}$, starting from 0 (corresponding to straight tracks), to an appropriate stopping point, we calculate $\\theta = \\phi - \\arccos(\\frac{r}{2r_0})$\n",
    "- Group all hits with the near $\\theta$ and some other features to a detected track by DBSCAN. Since $\\theta$ can take very large or small values, using $\\sin(\\theta)$ and $\\cos(\\theta)$ is better.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "_cell_guid": "e081740e-8169-4481-b1df-f5dd5488314f",
    "_uuid": "0bee86255243664f24e4bcf48af2228a3100a8b7"
   },
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#import hdbscan\n",
    "from scipy import stats\n",
    "from tqdm import tqdm\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from trackml_helper import *\n",
    "from analysis import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "_cell_guid": "e06d1ed7-5091-4d67-abb4-5984b137e2e6",
    "_uuid": "c2f70ae63abffcc09a534bb17fb89df8ffddb722",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def merge(cl1, cl2): # merge cluster 2 to cluster 1\n",
    "    d = pd.DataFrame(data={'s1':cl1,'s2':cl2})\n",
    "    d['N1'] = d.groupby('s1')['s1'].transform('count')\n",
    "    d['N2'] = d.groupby('s2')['s2'].transform('count')\n",
    "    maxs1 = d['s1'].max()+1\n",
    "    cond = np.where((d['N2'].values>d['N1'].values) & (d['N2'].values<25)) # Locate the hit with the new cluster> old cluster\n",
    "    s1 = d['s1'].values \n",
    "    s1[cond] = d['s2'].values[cond]+maxs1 # Assign all hits that belong to the new track (+ maxs1 to increase the label for the track so it's different from the original).\n",
    "    return s1\n",
    "\n",
    "def extract_good_hits(truth, submission):\n",
    "    tru = truth[['hit_id', 'particle_id', 'weight']].merge(submission, how='left', on='hit_id')\n",
    "    tru['count_both'] = tru.groupby(['track_id', 'particle_id']).hit_id.transform('count')    \n",
    "    tru['count_particle'] = tru.groupby(['particle_id']).hit_id.transform('count')\n",
    "    tru['count_track'] = tru.groupby(['track_id']).hit_id.transform('count')\n",
    "    return tru[(tru.count_both > 0.5*tru.count_particle) & (tru.count_both > 0.5*tru.count_track)]\n",
    "\n",
    "def fast_score(good_hits_df):\n",
    "    return good_hits_df.weight.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True: #tuning\n",
    "    path_to_train = \"/home/alexanderliao/data/Kaggle/competitions/trackml-particle-identification/train\"\n",
    "    event_prefix = \"event000001000\"\n",
    "    hits, cells, particles, truth = load_event(os.path.join(path_to_train, event_prefix))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Clusterer(object):\n",
    "    def __init__(self):                        \n",
    "        self.abc = []\n",
    "          \n",
    "    def initialize(self,dfhits):\n",
    "        self.cluster = range(len(dfhits))\n",
    "        self.curr_cluster = range(len(dfhits))\n",
    "        self.candidates = []\n",
    "        \n",
    "    def Clustering(self,\n",
    "                   dfh,\n",
    "                   coef,\n",
    "                   epsilon,\n",
    "                   min_samples=1,\n",
    "                   n_loop=180,\n",
    "                   stepii=0.000005,\n",
    "                   dz=5.5,\n",
    "                   step_dz=0.1,\n",
    "                   verbose=True): \n",
    "        merged_cluster = self.cluster\n",
    "        mm = 1\n",
    "        stepii = stepii\n",
    "        count_ii = 0\n",
    "        adaptive_eps_coefficient = 1\n",
    "        dz=dz\n",
    "        step_dz=step_dz\n",
    "        \n",
    "        \n",
    "        \n",
    "        #print(np.arange(0,step_dz,dz))\n",
    "        #for ii in np.arange(0, n_loop*stepii, stepii):   \n",
    "        for z0 in np.arange(0, dz, step_dz):    \n",
    "            #for z0 in np.arange(0, dz, step_dz):\n",
    "            count_ii = 0\n",
    "            for ii in np.arange(0, n_loop*stepii, stepii): \n",
    "                count_ii += 1\n",
    "\n",
    "                for jj in range(2):\n",
    "                    mm = mm*(-1)\n",
    "                    dfh=preprocess_hits(dfh, mm*z0)\n",
    "                    \n",
    "                    eps_new = epsilon + count_ii*adaptive_eps_coefficient*10**(-5)\n",
    "\n",
    "                    dfh['theta'] = dfh['phi'].values - np.nan_to_num(np.arccos(mm*ii*dfh['rt'].values))\n",
    "                    dfh['sina1'] = np.sin(dfh['theta'].values)\n",
    "                    dfh['cosa1'] = np.cos(dfh['theta'].values)\n",
    "\n",
    "                    ss = StandardScaler()\n",
    "                    dfs = ss.fit_transform(dfh[['sina1','cosa1','zdivrt','zdivr','xdivr','ydivr']].values) \n",
    "                    #dfs = scale_ignore_nan(dfh[['sina1','cosa1','zdivrt','zdivr','xdivr','ydivr']])\n",
    "                    dfs = np.multiply(dfs, coef)\n",
    "                    \n",
    "                    n_jobs = -1\n",
    "                    \n",
    "                    x = [eps_new,min_samples,n_jobs,dfs]\n",
    "                    #print(x)\n",
    "                    #p1=mp.Pool()\n",
    "                    #new_cluster=p1.map(DBSCAN_wrapper,x)\n",
    "                    new_cluster = DBSCAN_wrapper(x)\n",
    "                    merged_cluster = merge(merged_cluster, new_cluster)\n",
    "\n",
    "\n",
    "                if verbose == True:\n",
    "                    sub = create_one_event_submission(0, hits, merged_cluster)\n",
    "                    good_hits = extract_good_hits(truth, sub)\n",
    "                    score_1 = fast_score(good_hits)\n",
    "                    print('2r0_inverse:', ii*mm ,'. Score:', score_1)\n",
    "                        #clear_output(wait=True)\n",
    "\n",
    "            self.curr_cluster = merged_cluster\n",
    "            self.candidates.append(merged_cluster)\n",
    "            self.cluster = merge(merged_cluster,self.cluster)\n",
    "            \n",
    "def DBSCAN_wrapper(x):\n",
    "    return DBSCAN(eps=x[0],min_samples=x[1],metric='euclidean',n_jobs=x[2]).fit(x[3]).labels_\n",
    "                                          \n",
    "                    \n",
    "def create_one_event_submission(event_id, hits, labels):\n",
    "    sub_data = np.column_stack(([event_id]*len(hits), hits.hit_id.values, labels))\n",
    "    submission = pd.DataFrame(data=sub_data, columns=[\"event_id\", \"hit_id\", \"track_id\"]).astype(int)\n",
    "    return submission  \n",
    "\n",
    "def preprocess_hits(h,dz):\n",
    "    h['z'] =  h['z'].values + dz\n",
    "    h['r'] = np.sqrt(h['x'].values**2+h['y'].values**2+h['z'].values**2)\n",
    "    h['rt'] = np.sqrt(h['x'].values**2+h['y'].values**2)\n",
    "    h['phi'] = np.arctan2(h['y'].values,h['x'].values)\n",
    "    h['zdivrt'] = h['z'].values/h['rt'].values\n",
    "    h['zdivr'] = h['z'].values/h['r'].values\n",
    "    h['xdivr'] = h['x'].values / h['r'].values\n",
    "    h['ydivr'] = h['y'].values / h['r'].values\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False: #benchmark\n",
    "    c = [1.6,1.6,0.73,0.17,0.027,0.027]\n",
    "    model = Clusterer()\n",
    "    model.initialize(hits) \n",
    "    hits_with_dz = preprocess_hits(hits, 0)\n",
    "    model.Clustering(hits,coef=c,\n",
    "                           epsilon=0.0048,\n",
    "                           min_samples=1,\n",
    "                           n_loop=180,\n",
    "                           stepii=0.000005,\n",
    "                           verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2r0_inverse: 0.0 . Score: 0.05009114\n"
     ]
    }
   ],
   "source": [
    "# Clustering by varying \n",
    "#model = Clusterer()\n",
    "#model.initialize(hits) \n",
    "#c = [1.5,1.5,0.73,0.17,0.027,0.027] 0.52\n",
    "#c = [1.6,1.6,0.73,0.17,0.027,0.027] 0.529\n",
    "#c = [1.45,1.45,0.73,0.17,0.027,0.027] 0.523\n",
    "\n",
    "#[phi_coef,phi_coef,zdivrt_coef,zdivr_coef,xdivr_coef,ydivr_coef]\n",
    "min_samples_in_cluster = 2\n",
    "c = [1.6,1.6,0.73,0.17,0.027,0.027]\n",
    "model = Clusterer()\n",
    "model.initialize(hits) \n",
    "\n",
    "model.Clustering(  hits,\n",
    "                   coef=c,\n",
    "                   epsilon=0.0048,\n",
    "                   min_samples=1,\n",
    "                   n_loop=180,\n",
    "                   dz = 0.55,\n",
    "                   step_dz = 0.1,\n",
    "                   stepii=0.000005,\n",
    "                   verbose=True)\n",
    "\n",
    "submission = create_one_event_submission(0, hits, model.cluster)\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering by varying \n",
    "#model = Clusterer()\n",
    "#model.initialize(hits) \n",
    "\n",
    "# Preparing Submission\n",
    "if False:\n",
    "    for i in tqdm(range(62,125)):\n",
    "        path_to_train = \"/home/alexanderliao/data/Kaggle/competitions/trackml-particle-identification/test\"\n",
    "        event_prefix = \"event\"+str(i).zfill(9)\n",
    "        hits = load_event_hits(os.path.join(path_to_train, event_prefix))\n",
    "        c = [1.6,1.6,0.73,0.17,0.027,0.027] #[phi_coef,phi_coef,zdivrt_coef,zdivr_coef,xdivr_coef,ydivr_coef]\n",
    "        min_samples_in_cluster = 1\n",
    "\n",
    "        model = Clusterer()\n",
    "        model.initialize(hits) \n",
    "        hits_with_dz = preprocess_hits(hits, 0)\n",
    "        model.Hough_clustering(hits_with_dz,coef=c,epsilon=0.0048,min_samples=min_samples_in_cluster,\n",
    "                               n_loop=300,verbose=False)\n",
    "\n",
    "        if i == 62:\n",
    "            submission = create_one_event_submission(i, hits, model.cluster)\n",
    "        else:\n",
    "            submission = pd.concat([submission,create_one_event_submission(i, hits, model.cluster)])\n",
    "        print(submission)\n",
    "        if False: # O(n^2) if turned on\n",
    "            submission.to_csv('submission.csv')\n",
    "    print('\\n') \n",
    "    submission.to_csv('submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission\n",
    "df = submission.track_id.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "6967634/len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(63,124):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2df8584057590d42adef24a1286a92599f886df3"
   },
   "source": [
    "Now, let us see some analysis on the clustering result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d496bcbe21a377925f64c69de5a6c811c80d58a7"
   },
   "outputs": [],
   "source": [
    "pr = precision(truth,submission,min_hits=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fc582df7cf36f13b9ee0331572db305521e6336e"
   },
   "outputs": [],
   "source": [
    "pr = precision(truth,submission,min_hits=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8d22eb6747f01fa2af6bd2086cde122825e323a3"
   },
   "outputs": [],
   "source": [
    "pr = precision(truth,submission,min_hits=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "71aeb104416c23c2e57c45f7569939ff975780de"
   },
   "outputs": [],
   "source": [
    "pr = precision(truth,submission,min_hits=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "184b5b3541d8f8a82bdce122b3dc9b041af0d984"
   },
   "source": [
    "As one can see, long tracks have high precision, low lost weights. On the other hand, there are too many ghost short tracks. Then, we can use multi-stage clustering, using min_hits in DBSCAN for each stage (i.e., cluster long tracks first, then cluster short tracks with different parameters without touching the long tracks...)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "efd3174b9193b3180af1648d45d43db042e1fc55"
   },
   "source": [
    "Some other notes:\n",
    "\n",
    "+ Use too many loops can decrease the performance, as one can see from the log result above.\n",
    "\n",
    "+ No z-shifting is performed  (dz = 0), although the function preprocess already offer it. Some may want to use z-shifting right away just by change dz from 0 to any number between [-5.5, 5.5]\n",
    "\n",
    "+ Features are not optimized. Honestly, I am also stuck at searching for good features (and good weights). It would be very nice if someone secretly tell me those magic features :-).\n",
    "\n",
    "+ When $r/(2r_0) > 1$ or $< 1$, arccos is undefined, hence a warning appears (if running on local notebook). The problem, more importantly, is not about the warning. It is a technical issue: all hits with $r/(2r_0) > 1$ or $< 1$ MUST BE EXCLUDED from DBSCAN, because there will be NO track with that parameter pass through those hits. This can be done by some indexing techniques that I do not provide here. (DBSCAN uses a raw matrix to cluster, then we must be careful when exclude hits from the original full hit dataframe).\n",
    "\n",
    "KV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b8a951a9c4989186d8716a25e087fde36bcc30e2"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
