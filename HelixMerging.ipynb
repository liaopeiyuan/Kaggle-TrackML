{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from trackml.dataset import load_event\n",
    "from trackml.score import score_event\n",
    "\n",
    "#from utils.session import Session\n",
    "#from geometric.helix import HelixUnroll\n",
    "#from geometric.tools import merge_naive, reassign_noise, label_encode, hit_completeness\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.cluster import dbscan\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_encode(y):\n",
    "    return LabelEncoder().fit_transform(y)\n",
    "\n",
    "\n",
    "def reassign_noise(labels: np.ndarray, idx):\n",
    "    \"\"\"\n",
    "    assign noisy points (labeled with key_value such as -1 or 0) to their own clusters of size 1\n",
    "    \"\"\"\n",
    "    ret = labels.copy()\n",
    "    ret[idx] = np.arange(np.sum(idx)) + np.max(ret) + 1\n",
    "    return ret\n",
    "\n",
    "\n",
    "def merge_naive(pred_1, pred_2, cutoff=20):\n",
    "    \"\"\"\n",
    "    naive cluster merging:\n",
    "    iterate over hits; if a hit belongs to a larger cluster in pred_2, it is reassigned\n",
    "    \"\"\"\n",
    "    if pred_1 is None:\n",
    "        return pred_2\n",
    "    c1, c2 = Counter(pred_1), Counter(pred_2)  # track id -> track size\n",
    "    n1, n2 = np.vectorize(c1.__getitem__)(pred_1), np.vectorize(\n",
    "        c2.__getitem__)(pred_2)  # hit id -> track size\n",
    "    pred = pred_1.copy()\n",
    "    idx = (n2 > n1) & (n2 < cutoff)\n",
    "    pred[idx] = max(pred_1) + 1 + pred_2[idx]\n",
    "    return label_encode(pred)\n",
    "\n",
    "\n",
    "def merge_discreet(pred_1, pred_2, cutoff=21):\n",
    "    \"\"\"\n",
    "    discreet cluster merging (less likely to reassign points)\n",
    "    iterate over clusters in pred_2; np.sum(n1[idx]) < c2[track]**2 -> pred[idx] = d + track\n",
    "    this is self-documenting\n",
    "    \"\"\"\n",
    "    if pred_1 is None:\n",
    "        return pred_2\n",
    "    c1, c2 = Counter(pred_1), Counter(pred_2)  # track id -> track size\n",
    "    n1, n2 = np.vectorize(c1.__getitem__)(pred_1), np.vectorize(\n",
    "        c2.__getitem__)(pred_2)  # hit id -> track size\n",
    "    pred = reassign_noise(pred_1, n1 > cutoff)\n",
    "    pred_2 = reassign_noise(pred_2, n2 > cutoff)\n",
    "    n1[n1 > cutoff] = 1\n",
    "    n2[n2 > cutoff] = 1\n",
    "    d = max(pred) + 1\n",
    "    for track in c2:\n",
    "        if c2[track] < 3:\n",
    "            continue\n",
    "        idx = pred_2 == track\n",
    "        if np.sum(n1[idx]) < c2[track]**2:\n",
    "            pred[idx] = d + track\n",
    "    return label_encode(pred)\n",
    "\n",
    "\n",
    "def hit_completeness(df, idx, track_size=None):\n",
    "    \"\"\"\n",
    "    (the number of non-noisy hits in the idx) / (the total number of hits from all particles\n",
    "    that have at least 1 hit in the idx)\n",
    "    \"\"\"\n",
    "    if track_size is None:\n",
    "        track_size = df.groupby(\"particle_id\")[\"x\"].agg(\"count\")\n",
    "    num = (df.loc[idx, \"particle_id\"] != 0).sum()\n",
    "    all_particles = df.loc[idx, \"particle_id\"].unique().tolist()\n",
    "    if 0 in all_particles:\n",
    "        all_particles.remove(0)\n",
    "    denom = track_size[all_particles].sum()\n",
    "    return num / denom\n",
    "\n",
    "\n",
    "def track_completeness(df, idx):\n",
    "    \"\"\"\n",
    "    (number of tracks with all hits in the region) / (number of tracks that have at least 1 hit in the region)\n",
    "    idx is a boolean mask over the region\n",
    "    \"\"\"\n",
    "    all_particles = df.loc[idx, \"particle_id\"].unique().tolist()\n",
    "    if 0 in all_particles:\n",
    "        all_particles.remove(0)\n",
    "\n",
    "    agg_1 = df.loc[idx, :].groupby(\"particle_id\", sort=True)[\"x\"].agg(\"count\")\n",
    "    if 0 in agg_1:\n",
    "        agg_1.drop(0, inplace=True)\n",
    "    agg_2 = df.loc[df.particle_id.isin(all_particles), :].groupby(\n",
    "        \"particle_id\", sort=True)[\"x\"].agg(\"count\")\n",
    "    return np.mean(agg_1 == agg_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HelixUnroll(object):\n",
    "    def __init__(\n",
    "            self,\n",
    "            # helix-unrolling parameters\n",
    "            r3_func=lambda x, y, z: np.sqrt(x**2 + y**2 + z**2),\n",
    "            dz_func=lambda i: (-1)**(i+1) * (-7e-4 + i * 1e-5),\n",
    "            n_steps=150,\n",
    "            hidden_transform=lambda x: x * np.array([1.0, 1.0, 0.75]),\n",
    "            # cluster aggregation parameters\n",
    "            merge_func=merge_naive,\n",
    "            # dbscan parameters\n",
    "            eps_func=lambda i: 3.5e-3 + 5e-6 * i,\n",
    "            p=2,\n",
    "            dbscan_n_jobs=-1\n",
    "            ):\n",
    "        self.r3_func = r3_func\n",
    "        self.dz_func = dz_func\n",
    "        self.n_steps = n_steps\n",
    "        self.hidden_transform = hidden_transform  # transform the hidden space after scaling, before dbscan\n",
    "\n",
    "        self.merge_func = merge_func\n",
    "\n",
    "        self.eps_func = eps_func\n",
    "        self.p = p\n",
    "        self.dbscan_n_jobs = dbscan_n_jobs\n",
    "\n",
    "    def fit_predict(self, df, score_func=None, verbose=False,i):\n",
    "        df = df.copy()\n",
    "        df.loc[:, \"r3\"] = self.r3_func(df.x, df.y, df.z)\n",
    "\n",
    "        # df.loc[:, \"rs\"] = np.sqrt(df.x ** 2 + df.y ** 2 + df.z ** 2)  # radius in spherical coordinate system\n",
    "        df.loc[:, \"rt\"] = np.sqrt(df.x ** 2 + df.y ** 2)  # radius in cylindrical coordinate system\n",
    "        df.loc[:, \"a0\"] = np.arctan2(df.y, df.x)  # angle in cylindrical coordinate system\n",
    "        df.loc[:, \"z1\"] = df.z / df.rt  # monotonic with cot(psi)\n",
    "        # df.loc[:, \"z2\"] = df.z / df.rs TODO: 4 feature maybe? [1, 1, 0.4, 0.4]\n",
    "\n",
    "        pred = []\n",
    "        score_list = []\n",
    "\n",
    "        \n",
    "        dz = self.dz_func(i)\n",
    "        df.loc[:, \"a1\"] = df.a0 + dz * df.r3  # rotation, points with higher r3 are rotated to a larger degree\n",
    "        # convert angle to sin/cos -> more intuitive in Euclidean distance\n",
    "        # e.g. 2pi and 0 should be very close\n",
    "        df.loc[:, \"sina1\"] = np.sin(df.a1)\n",
    "        df.loc[:, \"cosa1\"] = np.cos(df.a1)\n",
    "        # scale the space\n",
    "        dfs = scale(df.loc[:, [\"sina1\", \"cosa1\", \"z1\"]])\n",
    "        # use hidden transformation methods to re-weight the features. Consider nonlinear transformations later.\n",
    "        dfs = self.hidden_transform(dfs)\n",
    "\n",
    "            \"\"\"\n",
    "            if score_func is not None:\n",
    "                # use a callback to customize scoring\n",
    "                step_score = score_func(pred)\n",
    "                score_list.append(step_score)\n",
    "                if verbose:\n",
    "                    print(str(i).rjust(3) + \": {:.6f}\".format(step_score))\n",
    "            \"\"\"\n",
    "        return dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Session(object):\n",
    "    \"\"\"\n",
    "    A highly integrated framework for efficient data loading, prediction submission, etc. in TrackML Challenge\n",
    "    (improved version of the official TrackML package)\n",
    "\n",
    "    Precondition: the parent directory must be organized as follows:\n",
    "    - train (directory)\n",
    "        - event000001000-cells.csv\n",
    "        ...\n",
    "    - test (directory)\n",
    "        - event000000001-cells.csv\n",
    "        ...\n",
    "    - detectors.csv\n",
    "    - sample_submission.csv\n",
    "    \"\"\"\n",
    "    # important constants to avoid spelling errors\n",
    "    HITS = \"hits\"\n",
    "    CELLS = \"cells\"\n",
    "    PARTICLES = \"particles\"\n",
    "    TRUTH = \"truth\"\n",
    "\n",
    "    def __init__(self, parent_dir=\"./\", train_dir=\"train/\", test_dir=\"test/\", detectors_dir=\"detectors.csv\",\n",
    "                 sample_submission_dir=\"sample_submission.csv\"):\n",
    "        \"\"\"\n",
    "        default input:\n",
    "        Session(\"./\", \"train/\", \"test/\", \"detectors.csv\", \"sample_submission.csv\")\n",
    "        Session(parent_dir=\"./\", train_dir=\"train/\", test_dir=\"test/\", detectors_dir=\"detectors.csv\", sample_submission_dir=\"sample_submission.csv\")\n",
    "        \"\"\"\n",
    "        self._parent_dir = parent_dir\n",
    "        self._train_dir = train_dir\n",
    "        self._test_dir = test_dir\n",
    "        self._detectors_dir = detectors_dir\n",
    "        self._sample_submission_dir = sample_submission_dir\n",
    "\n",
    "        if not os.path.isdir(self._parent_dir):\n",
    "            raise ValueError(\"The input parent directory {} is invalid.\".format(self._parent_dir))\n",
    "\n",
    "        # there are 8850 events in the training dataset; some ids from 1000 to 9999 are skipped\n",
    "        if os.path.isdir(self._parent_dir + self._train_dir):\n",
    "            self._train_event_id_list = sorted(\n",
    "                set(int(x[x.index(\"0\"):x.index(\"-\")]) for x in os.listdir(self._parent_dir + self._train_dir)))\n",
    "        else:\n",
    "            self._train_dir = None\n",
    "            self._train_event_id_list = []\n",
    "\n",
    "        if os.path.isdir(self._parent_dir + self._test_dir):\n",
    "            self._test_event_id_list = sorted(\n",
    "                set(int(x[x.index(\"0\"):x.index(\"-\")]) for x in os.listdir(self._parent_dir + self._test_dir)))\n",
    "        else:\n",
    "            self._test_dir = None\n",
    "            self._test_event_id_list = []\n",
    "\n",
    "        if not os.path.exists(self._parent_dir + self._detectors_dir):\n",
    "            self._detectors_dir = None\n",
    "\n",
    "        if not os.path.exists(self._parent_dir + self._sample_submission_dir):\n",
    "            self._sample_submission_dir = None\n",
    "\n",
    "    @staticmethod\n",
    "    def get_event_name(event_id):\n",
    "        return \"event\" + str(event_id).zfill(9)\n",
    "\n",
    "    def get_train_events(self, n=10, content=(HITS, TRUTH), randomness=True):\n",
    "        n = min(n, len(self._train_event_id_list))\n",
    "        if randomness:\n",
    "            event_ids = np.random.choice(self._train_event_id_list, size=n, replace=False).tolist()\n",
    "        else:\n",
    "            event_ids, = self._train_event_id_list[:n]\n",
    "            self._train_event_id_list = self._train_event_id_list[n:] + self._train_event_id_list[:n]\n",
    "\n",
    "        event_names = [Session.get_event_name(event_id) for event_id in event_ids]\n",
    "        return event_names, \\\n",
    "            (load_event(self._parent_dir + self._train_dir + event_name, content) for event_name in event_names)\n",
    "\n",
    "    def remove_train_events(self, n=10, content=(HITS, TRUTH), randomness=True):\n",
    "        \"\"\"\n",
    "        get n events from self._train_event_id_list:\n",
    "        if random, get n random events; otherwise, get the first n events\n",
    "        :return:\n",
    "         1. ids: event ids\n",
    "         2. an iterator that loads a tuple of hits/cells/particles/truth files\n",
    "        remove these train events from the current id list\n",
    "        \"\"\"\n",
    "        n = min(n, len(self._train_event_id_list))\n",
    "        if randomness:\n",
    "            event_ids = np.random.choice(self._train_event_id_list, size=n, replace=False).tolist()\n",
    "            for event_id in event_ids:\n",
    "                self._train_event_id_list.remove(event_id)\n",
    "        else:\n",
    "            event_ids, self._train_event_id_list = self._train_event_id_list[:n], self._train_event_id_list[n:]\n",
    "\n",
    "        event_names = [Session.get_event_name(event_id) for event_id in event_ids]\n",
    "        return event_names, \\\n",
    "            (load_event(self._parent_dir + self._train_dir + event_name, content) for event_name in event_names)\n",
    "\n",
    "    def remove_test_events(self, n=10, content=(HITS, CELLS), randomness=False):\n",
    "        n = min(n, len(self._test_event_id_list))\n",
    "        if randomness:\n",
    "            event_ids = np.random.choice(self._test_event_id_list, size=n, replace=False).tolist()\n",
    "            for event_id in event_ids:\n",
    "                self._test_event_id_list.remove(event_id)\n",
    "        else:\n",
    "            event_ids, self._test_event_id_list = self._test_event_id_list[:n], self._test_event_id_list[n:]\n",
    "        event_names = [Session.get_event_name(event_id) for event_id in event_ids]\n",
    "        return event_names, \\\n",
    "            (load_event(self._parent_dir + self._test_dir + event_name, content) for event_name in event_names)\n",
    "\n",
    "    def make_submission(self, predictor, path):\n",
    "        \"\"\"\n",
    "        :param predictor: function, predictor(hits: pd.DataFrame, cells: pd.DataFrame)->np.array\n",
    "         takes in hits and cells data frames, return a numpy 1d array of cluster ids\n",
    "        :param path: file path for submission file\n",
    "        \"\"\"\n",
    "        sub_list = []  # list of predictions by event\n",
    "        for event_id in self._test_event_id_list:\n",
    "            event_name = Session.get_event_name(event_id)\n",
    "\n",
    "            hits, cells = load_event(self._parent_dir + self._test_dir + event_name, (Session.HITS, Session.CELLS))\n",
    "            pred = predictor(hits, cells)  # predicted cluster labels\n",
    "            sub = pd.DataFrame({\"hit_id\": hits.hit_id, \"track_id\": pred})\n",
    "            sub.insert(0, \"event_id\", event_id)\n",
    "            sub_list.append(sub)\n",
    "        final_submission = pd.concat(sub_list)\n",
    "        final_submission.to_csv(path, sep=\",\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = Session(parent_dir=\"/home/alexanderliao/data/GitHub/Kaggle-TrackML/portable-dataset/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HelixClassifier(BaseEstimator, ClassifierMixin):\n",
    "\n",
    "     def __init__(self,helix,i,lo,hi):\n",
    "         self.helix = helix\n",
    "         self.i = i\n",
    "         self.lo = lo\n",
    "         self.hi = hi\n",
    "\n",
    "     def fit(self, X, y):\n",
    "         self.X_ = X\n",
    "         self.y_ = y \n",
    "         # Return the classifier\n",
    "         return self\n",
    "\n",
    "     def predict(self, X):    \n",
    "          X.loc[:, \"psi\"] = np.arctan2(np.sqrt(X.x ** 2 + X.y ** 2), np.abs(X.z))\n",
    "          idx = (X.psi >= np.deg2rad(self.lo)) & (X.psi < np.deg2rad(self.hi))\n",
    "          return dbscan(X= self.helix.fit_predict(X.loc[idx, :], self.i), eps=self.helix.eps_func(i), min_samples=1, metric=\"minkowski\", p=self.helix.p, n_jobs=self.helix.dbscan_n_jobs)[1]\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_events = 1\n",
    "preds=[];\n",
    "classifiers=[];\n",
    "clabels=[];\n",
    "voters=[];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "unroller = HelixUnroll(\n",
    "        r3_func=lambda x, y, z: np.sqrt(x ** 2 + y ** 2 + z ** 2),\n",
    "        dz_func=lambda i: (-1) ** (i + 1) * (-7e-4 + i * 1e-5),\n",
    "        n_steps=120,\n",
    "        hidden_transform=lambda x: x * np.array([1.0, 1.0, 0.75]),\n",
    "        merge_func=merge_naive,\n",
    "        eps_func=lambda i: 3.5e-3 + 5e-6 * i,\n",
    "        p=2,\n",
    "        dbscan_n_jobs=-1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(unroller.n_steps):\n",
    "    voter = HelixClassifier(unroller,i,0,20).fit([1],[2])\n",
    "    classifiers.append(voter)\n",
    "    clabels.append(str(i))\n",
    "    voters.append((str(i),voter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VotingClassifier(estimators=[('0', HelixClassifier(helix=<__main__.HelixUnroll object at 0x7fb17c9b68d0>, hi=20,\n",
      "        i=0, lo=0)), ('1', HelixClassifier(helix=<__main__.HelixUnroll object at 0x7fb17c9b68d0>, hi=20,\n",
      "        i=1, lo=0)), ('2', HelixClassifier(helix=<__main__.HelixUnroll object at 0x7fb17c9b68d0>, h...HelixClassifier(helix=<__main__.HelixUnroll object at 0x7fb17c9b68d0>, hi=20,\n",
      "        i=119, lo=0))],\n",
      "         flatten_transform=None, n_jobs=1, voting='hard', weights=None)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('0', HelixClassifier(helix=<__main__.HelixUnroll object at 0x7fb17c9b68d0>, hi=20,\n",
       "        i=0, lo=0)), ('1', HelixClassifier(helix=<__main__.HelixUnroll object at 0x7fb17c9b68d0>, hi=20,\n",
       "        i=1, lo=0)), ('2', HelixClassifier(helix=<__main__.HelixUnroll object at 0x7fb17c9b68d0>, h...HelixClassifier(helix=<__main__.HelixUnroll object at 0x7fb17c9b68d0>, hi=20,\n",
       "        i=119, lo=0))],\n",
       "         flatten_transform=None, n_jobs=1, voting='hard', weights=None)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assembly = VotingClassifier(estimators=voters, voting='hard')\n",
    "print(assembly)\n",
    "assembly.fit([1],[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "results=[]\n",
    "for hits, truth in s1.get_train_events(n=n_events, content=[s1.HITS, s1.TRUTH], randomness=True)[1]:\n",
    "     print(\"=\" * 120)\n",
    "     hits = hits.merge(truth, how=\"left\", on=\"hit_id\")\n",
    "     results.append(assembly.predict(hits))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   for hits, truth in s1.get_train_events(n=n_events, content=[s1.HITS, s1.TRUTH], randomness=True)[1]:\n",
    "        print(\"=\" * 120)\n",
    "        hits = hits.merge(truth, how=\"left\", on=\"hit_id\")\n",
    "        preds.append(subroutine_psi_slice(hits, 5, 10))\n",
    "        print(len(preds[0]))\n",
    "    \n",
    "    classifiers = []\n",
    "    i = 0\n",
    "    for pred in preds:\n",
    "        for res in pred:\n",
    "            voter = DummyClassifier(data=res).fit([1],[2])\n",
    "            classifiers.append((str(i),voter))\n",
    "            i = i + 1\n",
    "        vote = VotingClassifier(estimators=classifiers, voting='hard')\n",
    "        vote.fit([1,2],[2])\n",
    "        class_res = vote.predict([1]*17650)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
